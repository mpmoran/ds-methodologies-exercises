{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Project\n",
    "## Clustering Module\n",
    "By Michael P. Moran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Project Planning](#project-planning)\n",
    "1. [Prepare Environment](#prepare-environment)\n",
    "    1. [Sub paragraph](#subparagraph1)\n",
    "1. [Acquisition](#acquisition)\n",
    "1. [Preparation](#preparation)\n",
    "1. [Exploration](#exploration)\n",
    "1. [Modeling](#modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- SQL query\n",
    "    - [X] only include properties with a transaction in 2016 &/or 2017 (along with zestimate error and date of transaction).\n",
    "- Removing lots\n",
    "    - [ ] find a ratio between taxlandvalue and lotsizesquarefeet to exclude lots (there are some with 70k taxappraisalvalue and a lotsize of 30-40k square feet\n",
    "    - [X] fill taxdelinquencyflag with N for the NaNs.\n",
    "    - [ ] what to do with taxdelinquencyyear? maybe combine the flag with the year to create a variable that reflects how long it has been delinquent and put a 0 for those that are not delinquent\n",
    "    - [ ] combine calculatedsqft and lotsizesqft???\n",
    "    - [ ] combine bedroomcnt and bathroomcnt?\n",
    "    - [ ] encode delinquency column\n",
    "    \n",
    "### Presentation\n",
    "- [ ] Topic will be 3 key takeaways from project\n",
    "    - e.g., I created this really cool function\n",
    "    - e.g., or garagecnt is determining the poolcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning <a name=\"project-planning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary & Domain Knowledge\n",
    "\n",
    "- regionidcounty\n",
    "    - 3101 --- 6037 (Los Angeles)\n",
    "    - 1286 --- 6059 (Orange County)\n",
    "    - 2061 --- 6111 (Ventura County) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA = 3101\n",
    "ORANGE = 1286\n",
    "VENTURA = 2061"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Low calculatedsqft is correlated with a higher logerror\n",
    "1. Low lotsizesqft is correlated with a higher logerror\n",
    "1. Low taxvaluedollarcnt is correlated with a higher logerror\n",
    "1. Low bedroomcnt is correlated with a higher logerror\n",
    "    - 2 to 4 bedrooms have higher logerror\n",
    "1. Low bathroomcnt is correlated with a higher logerror\n",
    "    - 1 to 3 bathrooms have higher logerror\n",
    "1. bedroomcnt and bathroomcnt are positively correlated\n",
    "1. calculatedsqft and taxvaluedollarcnt are positively correlated\n",
    "    - Yes. There is a 0.6 correlation coefficient\n",
    "1. lotsizesqft and taxvaluedollarcnt are positively correlated\n",
    "    - No. Tax value actually goes down with bigger lots (are these lots without houses??)\n",
    "1. lotsizesqft is not driving logerror because it is really high for condos (I believe it includes the whole development the condo is on) but the logerror is low for them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts & Questions\n",
    "\n",
    "- remove the condominums and planned unit developments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment <a name=\"prepare-environment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire_zillow\n",
    "import prepare_zillow\n",
    "import explore_zillow\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload modules to capture changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_zillow = reload(acquire_zillow)\n",
    "prepare_zillow = reload(prepare_zillow)\n",
    "explore_zillow = reload(explore_zillow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition <a name=\"acquisition\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire_zillow.get_zillow_from_csv(\"zillow_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation <a name=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_zillow.summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the first function that returns missing value totals by column: Does the attribute have enough information (i.e. enough non-null values) to be useful? Choose your cutoff and remove columns where there is not enough information available. Document your cutoff and your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop certain columns**\n",
    "\n",
    "- Same information\n",
    "    - calculatedbathnbr\n",
    "        - because it has 99% of the same values as bathroomcnt\n",
    "    - finishedsquarefeet12\n",
    "        - because it has the same information as calculatedfinishedsquarefeet except for 9 rows.\n",
    "    - structuretaxvaluedollarcnt\n",
    "        - because it has the same info as taxvaluedollarcnt\n",
    "        \n",
    "- 100% or near 100% missing values\n",
    "    - architecturalstyledesc\n",
    "    - basementsqft\n",
    "    - buildingclassdesc\n",
    "    - decktypeid\n",
    "    - finishedfloor1squarefeet\n",
    "    - finishedsquarefeet13\n",
    "\t- finishedsquarefeet15\n",
    "    - finishedsquarefeet50\n",
    "    - finishedsquarefeet6\n",
    "\t- fireplacecnt\n",
    "    - fireplaceflag\n",
    "    - garagecarcnt\n",
    "    - garagetotalsqft\n",
    "    - hashottuborspa\n",
    "    - numberofstories\n",
    "\t- poolsizesum\n",
    "    - pooltypeid10\n",
    "    - pooltypeid2\n",
    "\t- storydesc\n",
    "    - taxdelinquencyyear (not sure how to impute this one)\n",
    "    - threequarterbathnbr\n",
    "    - typeconstructiondesc\n",
    "\t- yardbuildingsqft17\n",
    "    - yardbuildingsqft26\n",
    "    \n",
    "- too difficult to impute\n",
    "    - regionidneighborhood (almost 50% missing; not sure how to impute this)\n",
    "    \n",
    "- inferior information\n",
    "    - fullbathcnt\n",
    "        - because bathroomcnt has more fine-grained information; it includes half bathrooms, etc.\n",
    "\n",
    "- unsure what to do with\n",
    "    - airconditioningdesc\n",
    "    - heatingorsystemdesc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute 0 for certain columns**\n",
    "\n",
    "- hashottuborspa\n",
    "- poolcnt\n",
    "- pooltypeid7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute values for certain columns**\n",
    "- most frequent value\n",
    "    - buildingqualitytypeid (7)\n",
    "    - propertyzoningdes (LAR1)\n",
    "    - regionidcity (12447)\n",
    "    - regionidzip (?)\n",
    "    - yearbuilt (1950)\n",
    "- linear regression\n",
    "    - lotsizesquarefeet\n",
    "- constant\n",
    "    - taxdelinquencyflag (\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows**\n",
    "- Those with NaN in columns with only few NaNs (not worth the time to impute)\n",
    "    - taxvaluedollarcnt\n",
    "    - landtaxvaluedollarcnt\n",
    "    - taxamount\n",
    "    - censustractandblock\n",
    "    \n",
    "those with nan in landtaxvaluedollarcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_zillow.df_missing_vals_by_col(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prepare function to do everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_zillow.prepare_zillow(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add column with abs of logerror**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"logerror_abs\"] = df.logerror.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration  <a name=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bin logerror**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by explicit bins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)], closed=\"left\")\n",
    "# # df[\"logerror_bin\"] = pd.cut(df.logerror_abs, bins=6, labels=[0, 1, 2, 3, 4, 5])\n",
    "# logerror_bin = pd.cut(df.logerror_abs, bins=bins)\n",
    "# intervals_to_labels = {str(index): i for i, index in enumerate(bins)}\n",
    "# # df_tmp.drop(columns=\"logerror_bin\")\n",
    "# logerror_bin_label = logerror_bin.apply(lambda x: intervals_to_labels[str(x)])\n",
    "# # for b, l in bins_to_labels:\n",
    "# #     df[df.logerror_bin == b][\"logerror_bin\"] = i\n",
    "# # df.logerror_bin.value_counts(dropna=False)\n",
    "# # # df = df.astype({\"logerror_bin\": int})\n",
    "# df[\"logerror_bin\"] = logerror_bin_label\n",
    "df[\"logerror_bin\"] = explore_zillow.series_bin_with_labels(df.logerror_abs, bins, (0, 1, 2, 3, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by quartile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"logerror_bin_quart\"] = pd.qcut(df.logerror_abs, q=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create sample of df for exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=25_000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create lists holding column names for continuous and categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = [\"calculatedfinishedsquarefeet\",\n",
    "                \"latitude\", \"longitude\", \"lotsizesquarefeet\",\n",
    "                \"yearbuilt\", \"taxvaluedollarcnt\", \"landtaxvaluedollarcnt\", \"logerror\"]\n",
    "\n",
    "contin_and_cat_cols = [\"bathroomcnt\", \"bedroomcnt\", \"poolcnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.df_plot_numeric(df_sample, continuous_cols, \"logerror_bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.df_jitter_plot(df_sample, contin_and_cat_cols, continuous_cols, \"logerror_bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relplot_num_and_cat(df: pd.DataFrame, x: str, y: str, hue: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Write a function that will use seaborn's relplot to plot 2 numeric (ordered) variables\n",
    "#     and 1 categorical variable. It will take, as input, a dataframe, column name indicated\n",
    "#     for each of the following: x, y, & hue.\n",
    "#     \"\"\"\n",
    "#     sns.relplot(x=x, y=y, hue=hue, data=df, alpha=0.8)\n",
    "#     plt.show\n",
    "\n",
    "# relplot_num_and_cat(df_sample[df_sample.logerror_bin != bins[0]] , \"longitude\", \"latitude\", \"logerror_bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def swarmplot_num_and_cat(df: pd.DataFrame, X: str, Y: list, hue: str=None) -> None:\n",
    "#     \"\"\"\n",
    "#     Write a function that will take, as input, a dataframe, a categorical column name,\n",
    "#     and a list of numeric column names. It will return a series of subplots: a swarmplot\n",
    "#     for each numeric column. X will be the categorical variable.\n",
    "#     \"\"\"\n",
    "#     cols = 3\n",
    "#     rows = round(len(Y) / cols) if len(Y) // cols > 0 else 1\n",
    "    \n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     for i, y in enumerate(Y):\n",
    "#         plt.subplot(rows, cols, i + 1)\n",
    "#         sns.swarmplot(x=X, y=y, data=df, hue=hue, palette=\"Set2\")\n",
    "#     plt.plot\n",
    "\n",
    "# swarmplot_num_and_cat(df_sample.sample(n=10_000), \"logerror_bin\",\n",
    "#                       [\"calculatedfinishedsquarefeet\", \"lotsizesquarefeet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosstab_cat(df: pd.DataFrame, cols: list) -> None:\n",
    "    \"\"\"\n",
    "    Write a function that will take a dataframe and a list of categorical columns to plot\n",
    "    each combination of variables in the chart type of your choice.\n",
    "    \"\"\"\n",
    "    for outer in cols:\n",
    "        for inner in cols:\n",
    "            if outer == inner:\n",
    "                continue\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            ct = pd.crosstab(df[outer], df[inner], margins=True)#.apply(lambda r: r/r.sum(), axis=1)\n",
    "            sns.heatmap(ct, cmap=\"YlGnBu\", annot=True, cbar=False, fmt=\".2f\")\n",
    "            #print(pd.crosstab(df[outer], df[inner], margins=True).apply(lambda r: r/r.sum(), axis=1))\n",
    "            plt.show()\n",
    "    \n",
    "crosstab_cat(df_sample, contin_and_cat_cols + [\"logerror_bin\"])\n",
    "#pd.crosstab(df.logerror_bin, df.bathroomcnt, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JointPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logerror among conties\n",
    "**Is logerror significantly different for properties in Los Angeles County vs\n",
    "Orange County (or Ventura County)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LA v. Orange**\n",
    "- H0: logerror is not different for properties in LA County v. Orange County\n",
    "    - Reject the null hypothesis. There is a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.regionidcounty.value_counts()\n",
    "logerror_la = df[df.regionidcounty == LA].logerror\n",
    "logerror_orange = df[df.regionidcounty == ORANGE].logerror\n",
    "logerror_ventura = df[df.regionidcounty == VENTURA].logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_ttest(logerror_la, logerror_orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LA v. Ventura**\n",
    "- H0: logerror is not different for properties in LA County v. Ventura County\n",
    "    - Reject the null hypothesis. There is a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_ttest(logerror_la, logerror_ventura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orange v. Ventura**\n",
    "- H0: logerror is not different for properties in Orange County v. Ventura County\n",
    "    - Fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_ttest(logerror_orange, logerror_ventura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "There are significant differences in logerror when comparing LA County to the other two. However, there is no difference in logerror between Orange and Ventura counties. So, I should include this variable as a feature but bin them based on whether they are in LA County or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logerror based on tax delinquency status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is logerror significantly different for properties that are delinquent on their taxes vs those that are not?**\n",
    "- H0: There is no differnece in logerror for properties that are delinquent v. those that are not\n",
    "    - Reject the null hypothesis. There is a significant difference in logerror between houses that are delinquent and those that are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logerror_delinq = df[df.taxdelinquencyflag == \"Y\"].logerror\n",
    "logerror_not_delinq = df[df.taxdelinquencyflag == \"N\"].logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_ttest(logerror_delinq, logerror_not_delinq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "I will include taxdelinquencyflag as a feature because there are significant differences in logerror for properties that are delinquent v. those that are not. A possible theory is that a tax delinquent status indicates a possible flipped house, which could cause a significant change in the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logerror based on yearbuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is logerror significantly different for properties built prior to 1960 than those built later than 2000?**\n",
    "- H0: There is no difference in logerror between properties built before 1960 and those built later than 2000\n",
    "     - Reject the null hypothesis. There is a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logerror_pre1960 = df[df.yearbuilt < 1960].logerror\n",
    "logerror_post2000 = df[df.yearbuilt > 2000].logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_ttest(logerror_pre1960, logerror_post2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "I may want to include yearbuilt as a feature and bin it. Or I may want to create separate models based on when the house was built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi2 Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are many discrete variables, you can the chi-squared test to test proportions. If you split logerror into quartiles, you can expect the overall probability of falling into a single quartile to be 25%. Now, add another variable, like bedrooms (and you can bin these if you want fewer distinct values) and compare the probabilities of bedrooms with logerror quartiles. See the example in the Classification_Project notebook we reviewed on how to implement chi-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bedroomcnt v. logerror\n",
    "\n",
    "H0: The bins for bedroom count and absolute value of logerror are independent\n",
    "    - Reject the null hypothesis. There are not independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bin bedroomcnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedroomcnt_bins = pd.IntervalIndex.from_tuples([(0, 2), (2, 4), (4, 6), (6, 100)], closed=\"right\")\n",
    "# df[\"bedroomcnt_bin\"] = explore_zillow.series_bin_with_labels(df.bedroomcnt, bedroomcnt_bins, )\n",
    "df[\"bedroomcnt_bin\"] = pd.cut(df.bedroomcnt, bins=bedroomcnt_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_chi2_test(df.bedroomcnt_bin, df.logerror_bin_quart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "Bedroom count and logerror are related and have some dependency on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bathroomcnt v. logerror\n",
    "\n",
    "H0: The bins for bathroom count and absolute value of logerror are independent\n",
    "    - Reject the null hypothesis. There are not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathroomcnt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bin bathroomcnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathroomcnt_bins = pd.IntervalIndex.from_tuples([(0, 2), (2, 4), (4, 6), (6, 8), (8, 100)], closed=\"right\")\n",
    "# df[\"bedroomcnt_bin\"] = explore_zillow.series_bin_with_labels(df.bedroomcnt, bedroomcnt_bins, )\n",
    "df[\"bathroomcnt_bin\"] = pd.cut(df.bathroomcnt, bins=bathroomcnt_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_zillow.series_chi2_test(df.bathroomcnt_bin, df.logerror_bin_quart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "Bathroom count and logerror are related and have some dependency on each other, and it's higher than for bedroom count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logerror_abs alone\n",
    "\n",
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_elbow(X: pd.DataFrame, nclusters_width, **kwargs):\n",
    "    intertias = []\n",
    "    nclusters_range = range(1, nclusters_width + 1)\n",
    "    for n in nclusters_range:\n",
    "        kmeans = KMeans(n_clusters=n, **kwargs)\n",
    "        kmeans.fit(X)\n",
    "        intertias.append(kmeans.inertia_)\n",
    "    \n",
    "    kmeans_perf = pd.DataFrame(list(zip(nclusters_range, intertias)), columns=['n_clusters', 'ssd'])\n",
    "\n",
    "    plt.scatter(kmeans_perf.n_clusters, kmeans_perf.ssd)\n",
    "    plt.plot(kmeans_perf.n_clusters, kmeans_perf.ssd)\n",
    "\n",
    "    plt.xticks(nclusters_range)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Sum of Squared Distances')\n",
    "    plt.title('The elbow method')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[[\"logerror_abs\"]]\n",
    "\n",
    "# intertias = []\n",
    "# for n in range(1, 11):\n",
    "#     kmeans = KMeans(n_clusters=n)\n",
    "#     kmeans.fit(X)\n",
    "#     intertias.append(kmeans.inertia_)\n",
    "    \n",
    "# kmeans_perf = pd.DataFrame(list(zip(range(1, 11), intertias)), columns=['n_clusters', 'ssd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(kmeans_perf.n_clusters, kmeans_perf.ssd)\n",
    "# plt.plot(kmeans_perf.n_clusters, kmeans_perf.ssd)\n",
    "\n",
    "# plt.xticks(range(1, 11))\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Sum of Squared Distances')\n",
    "# plt.title('The elbow method')\n",
    "# plt.show()\n",
    "\n",
    "kmeans_elbow(df[[\"logerror_abs\"]], 8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_fit_and_predict(X: pd.DataFrame, **kwargs) -> np.ndarray:\n",
    "    kmeans = KMeans(**kwargs)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.predict(X), kmeans.labels_, kmeans.inertia_\n",
    "\n",
    "df['cluster_target'], labels, interia = kmeans_fit_and_predict(df[[\"logerror_abs\"]], n_clusters=4, random_state=123)\n",
    "#     X = df[[\"logerror_abs\"]]\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=4)\n",
    "#     kmeans.fit(X)\n",
    "#     df['logerror_abs_cluster'] = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=X, x='logerror_abs', y=0, hue='logerror_abs_cluster', legend=\"full\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_elbow(df[[\"latitude\", \"longitude\"]], 8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latlong_cluster'], labels, interia = kmeans_fit_and_predict(df[[\"latitude\", \"longitude\"]], n_clusters=4, random_state=123)\n",
    "#     X = df[[\"logerror_abs\"]]\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=4)\n",
    "#     kmeans.fit(X)\n",
    "#     df['logerror_abs_cluster'] = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.relplot(data=df, x='longitude', y=\"latitude\", legend=\"full\", hue='latlong_cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.latlong_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df.latlong_cluster, prefix=\"latlong_cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From PairPlot\n",
    "    - logerror v. others\n",
    "        - logerror is worse for homes with a relatively small calculatedsqft and lotsizesqft\n",
    "        - logerror is worse in the middle of the latitude and longitude values\n",
    "        - logerror is worse for homes with a relatively low taxvaluedollarcnt, landtaxvaluedollarcnt, and structuretaxvaluedollarcnt\n",
    "        - worse for homes with fewer bedrooms and bathrooms\n",
    "    - taxvaluedollarcnt\n",
    "        - all the variables like this one (landtaxvaluedollarcnt, structuretaxvaluedollarcnt) have similar scatterplots when compared to other variables. They appear to be giving the same information.\n",
    "    - calculatedfinishedsqft\n",
    "        - slight correlation with yearbuilt\n",
    "        - slight correlation with taxvaluedollarcnt (0.6 Pearson R - see HeatMap)\n",
    "    - bathroomcnt\n",
    "        - positive correlation between calculatedfinishedsquarefeet\n",
    "    - bedroomcnt\n",
    "        - positive correlation between calculatedfinishedsquarefeet\n",
    "    - latitude\n",
    "        - houses near the middle of the latitude range have a higher square footage and taxvalue\n",
    "    - lotsizesquareft\n",
    "        - as square footage increases, taxvalue appears to decrease (a litle odd?)\n",
    "        - bedroomcnt and bathroomcnt appear to decrease as well\n",
    "- From RelPlot\n",
    "    - Houses with a logerror > 1 appear to form clusters based on longitude and latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURES**\n",
    "- [ ] **standardize all dependent variables, including binned ones (have bins be 0.1, 0.2, 0.3, etc)\n",
    "- regionidcounty\n",
    "    - [ ] Create dummy variable of la_county where 1 is yes and 0 is no.\n",
    "- taxdelinquencyflag\n",
    "- yearbuilt\n",
    "    - [ ] Create a new variable that reflects the age of the house from 2017. Maybe bin the houses by 20 year intervals\n",
    "- bedroomcnt + bathroomcnt\n",
    "    - [ ] combine bedroomcnt and bathroomcnt and bin them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
